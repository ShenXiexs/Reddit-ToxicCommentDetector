---
title: "Reddit_Scraping"
author: "Samxie"
date: "2025-03-25"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(stringr)
```

```{r}
# 获取文件夹中的所有CSV文件
csv_files <- list.files(path = folder_path, pattern = "^authors_part_\\d+\\.csv$", full.names = TRUE)

# 读取并合并所有CSV文件，确保文件顺序，并确保列名为 "author"
all_data <- do.call(rbind, lapply(csv_files, function(file) {
  df <- read.csv(file)
  colnames(df) <- "author"  # 确保每个文件的列名为 "author"
  return(df)
}))

# 查看合并后的数据（可选）
print(head(all_data))

# 计算每份数据的行数（平均分成5份）
num_rows <- nrow(all_data)
rows_per_group <- num_rows %/% 5
remainder <- num_rows %% 5

# 根据行数平均分配数据
split_data <- list()
start_row <- 1

for (i in 1:5) {
  end_row <- start_row + rows_per_group - 1
  if (i <= remainder) {
    end_row <- end_row + 1  # 如果有余数，当前组多分一行
  }
  
  split_data[[i]] <- all_data[start_row:end_row, ]
  start_row <- end_row + 1
}

# 输出合并后的数据到5个CSV文件，并确保列名为 "author"
for (i in 1:5) {
  output_filename <- paste0("/Users/samxie/Research/HEC/Reddit Scrape Python 0326/shen list/authors_merge_", i, ".csv")
  write.csv(split_data[[i]], output_filename, row.names = FALSE)
  cat("Saved:", output_filename, "\n")
}

print("Task completed!")
```
